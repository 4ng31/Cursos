{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -*- coding: utf-8 -*-\n",
      "\"\"\"\n",
      "    classifier.py\n",
      "    ~~~~~~~~~~~~~\n",
      " \n",
      "    Implementations of \n",
      "        KNN,\n",
      "        multivariate gaussian Bayes learned via MLE, and\n",
      "        softmax logistic regression classifiers learned via gradien descent\n",
      "    Sketch-level stuff, not totally turn-key at this point\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "from collections import Counter, defaultdict\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "class KNNClassifier(object):\n",
      "    \"\"\"Evaluate a test data point relative to some training data\n",
      "       and return the most probably class based on the nearest K points\n",
      "       by Euclidean distance\"\"\"\n",
      "    def __init__(self, k, x_train, y_train):\n",
      "\n",
      "        self.k = k\n",
      "        self.x_train = np.array(x_train)\n",
      "        self.y_train = np.array(y_train)\n",
      "\n",
      "    def classify(self, x_test):\n",
      "        # find vectors from each training point to the test point\n",
      "        diff_vecs = self.x_train - x_test\n",
      "        # find norms of these difference vectors\n",
      "        row_norms = np.linalg.norm(diff_vecs, axis=1)\n",
      "        # let's find the vectors having distance <= cutoff\n",
      "        neighbor_indices = row_norms.argpartition(self.k)[:self.k]\n",
      "        votes = Counter(np.take(y_train, neighbor_indices))\n",
      "        # From docs for .most_common():\n",
      "        #  Elements with equal counts are ordered arbitrarily\n",
      "        return votes.most_common(1)[0][0]\n",
      "\n",
      "\n",
      "class ConfusionMatrix(object):\n",
      "    \"\"\"nested dictionaries for storing test values\"\"\"\n",
      "    def __init__(self):\n",
      "        super(ConfusionMatrix, self).__init__()\n",
      "        self._data = defaultdict(lambda: defaultdict(lambda: 0))\n",
      "        self._n = 0\n",
      "\n",
      "    def update(self, true, pred):\n",
      "        self._data[true][pred] += 1\n",
      "        self._n += 1\n",
      "\n",
      "    def accuracy(self):\n",
      "        return sum([\n",
      "            self._data[i].get(i, 0)\n",
      "            for i in range(int(max(self._data.keys())))\n",
      "        ]) * (1. / self._n)\n",
      "\n",
      "\n",
      "def get_confusion_matrix(classifier, x_test, y_test):\n",
      "    confusion_mat = ConfusionMatrix()\n",
      "    errors = []\n",
      "    for test_x, test_y in zip(x_test, y_test):\n",
      "        pred = classifier.classify(test_x)\n",
      "        if isinstance(test_y, list):\n",
      "            true = test_y[0]\n",
      "        else:\n",
      "            true = test_y\n",
      "        confusion_mat.update(true, pred)\n",
      "        if true != pred:\n",
      "            errors.append([test_x, true, pred])\n",
      "    return confusion_mat, errors\n",
      "\n",
      "\n",
      "def _fit_multivariate_normal(x):\n",
      "    # From numpy docs for np.mean:\n",
      "    #   The arithmetic mean is the sum of the elements along the axis divided by the number of elements.\n",
      "    #   axis: Axis along which the means are computed\n",
      "    # My interpretation and reason for use:\n",
      "    # So we use np.mean to efficiently compute the mean 'vertically',\n",
      "    # summing values for each dimension and dividing by the number of rows\n",
      "    sample_mean = np.mean(x, axis=0)\n",
      "    # From numpy docs for np.cov:\n",
      "    #   If we examine N-dimensional samples, X = [x_1, x_2, ... x_N]^T, \n",
      "    #   then the covariance matrix element C_{ij} is the covariance \n",
      "    #   of x_i and x_j. The element C_{ii} is the variance of x_i.\n",
      "    #   bias : int, optional\n",
      "    #           Default normalization is by ``(N - 1)``, where ``N`` is the number of\n",
      "    #           observations given (unbiased estimate). If `bias` is 1, then\n",
      "    #           normalization is by ``N``.\n",
      "    # My interpretation and reason for use:\n",
      "    # So we can efficiently compute covariance by viewing the data matrix\n",
      "    # as a collection of samples of each dimension, each stored on a column.\n",
      "    # Construct a covariance matrix by finding the covariance between these column\n",
      "    # samples, or, for the diagonal entries, the variance of each sample.\n",
      "    # Because we are estimating by maximum likelihood, use bias=1\n",
      "    # Essentially just calls (dot(X, X.T) / N).squeeze() with nice type checking\n",
      "    sample_cov = np.cov(x, bias=True, rowvar=False)\n",
      "    # Create an instance of scipy.stats.multivariate normal with these parameters\n",
      "    # We'll just use its PDF method\n",
      "    return stats.multivariate_normal(mean=sample_mean, cov=sample_cov)\n",
      "\n",
      "\n",
      "class MultivariateNormalClassifier(object):\n",
      "    \"\"\"Evaluate a test data point relative to some training data\n",
      "       by fitting a multivariate normal distribution to the training\n",
      "       data of each class and assessing the density of each class conditional\n",
      "       density function at the location of the test point.\"\"\"\n",
      "    def __init__(self, x_train, y_train):\n",
      "        class_labels = set([y for x in y_train for y in x])\n",
      "        self.x_train = np.array(x_train)\n",
      "        self.y_train = np.array(y_train)\n",
      "        # IN THIS CASE, we know in advance that all classes are present in equal number\n",
      "        # in the training data, so we drop the p(class_k) term from the prediction\n",
      "        self._gaussians = {\n",
      "            label: _fit_multivariate_normal(\n",
      "                np.array([x_train[i] for i in np.where(np.array(y_train) == label)[0]])\n",
      "            )\n",
      "            for label in class_labels\n",
      "        }\n",
      "\n",
      "    def classify(self, test_x, verbose=False):\n",
      "        # use the pdf method from the multivariate_normal objects in self._gaussians\n",
      "        # the meat of the implementation is:\n",
      "        #   const * exp ( -0.5 * (\n",
      "        #       rank * _LOG_2PI + log_det_cov\n",
      "        #       + np.sum(np.square(np.dot(dev, prec_U)), axis=-1)))\n",
      "        # where prec_U, rank, log_det_cov come from symmetric eigendecomposition of precision matrix\n",
      "        # (pseudo-inverse of covariance matrix) and dev is the deviation of x from the mean\n",
      "        densities = [\n",
      "            (label, mvn.pdf(test_x))\n",
      "            for label, mvn in self._gaussians.iteritems()\n",
      "        ]\n",
      "        densities.sort(key=lambda r: -r[1])\n",
      "        if verbose:\n",
      "            return densities[0][0], densities\n",
      "        else:\n",
      "            return densities[0][0]\n",
      "\n",
      "\n",
      "def _gradient(k, w_container, x_train, y_train):\n",
      "    n_grad = [\n",
      "        np.array(train_x) * (\n",
      "            ((1 if train_y[0] == k else 0) -\n",
      "            (\n",
      "                np.exp(np.dot(np.transpose(train_x), w_container[k])) /\n",
      "                sum([\n",
      "                    np.exp(np.dot(np.transpose(train_x), w_container[j])) \n",
      "                    for j in w_container\n",
      "                    ])\n",
      "                )\n",
      "            ))\n",
      "        for train_x, train_y in zip(x_train, y_train)\n",
      "    ]\n",
      "\n",
      "    return np.sum(n_grad, axis=0)\n",
      "\n",
      "\n",
      "def _loglikelihood(w_container, x_train, y_train):\n",
      "    n_log_likelihood = [\n",
      "        np.dot(np.transpose(train_x), w_container[train_y[0]]) - \n",
      "        np.log(\n",
      "                sum([\n",
      "                    np.exp(np.dot(np.transpose(train_x), w_container[j])) \n",
      "                    for j in w_container\n",
      "                    ])\n",
      "            )\n",
      "        for train_x, train_y in zip(x_train, y_train)\n",
      "    ]\n",
      "    return sum(n_log_likelihood)\n",
      "\n",
      "\n",
      "def _iterate(w_container, x_train, y_train, nu=.1 / 5000):\n",
      "    w_container = _update_w_container((w_container, x_train, y_train, nu))\n",
      "    return w_container, _loglikelihood(w_container, x_train, y_train)\n",
      "\n",
      "\n",
      "def get_likelihood_trace(w_container, x_train, y_train):\n",
      "    \"\"\"\n",
      "    For plotting likelihood across iterations\n",
      "    \"\"\"\n",
      "    likelihood_trace = []\n",
      "    for i in range(1000):\n",
      "        w_container, log_likelihood = _iterate(w_container, x_train, y_train)\n",
      "        likelihood_trace.append(log_likelihood)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}