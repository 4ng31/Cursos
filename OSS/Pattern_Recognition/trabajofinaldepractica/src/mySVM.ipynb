{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Modules\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from numpy import linalg\n",
      "import scipy.io as spio\n",
      "from sklearn import preprocessing ## Para estandarizar y normalizar\n",
      "from sklearn.cross_validation import train_test_split ## Para partir el set de datos\n",
      "from sklearn import cross_validation ## Validacion cruzada\n",
      "import cvxopt\n",
      "import cvxopt.solvers\n",
      "\n",
      "def linear_kernel(x1, x2):\n",
      "    return np.dot(x1, x2)\n",
      "\n",
      "def polynomial_kernel(x, y, p):\n",
      "    return (1 + np.dot(x, y)) ** p\n",
      "\n",
      "def gaussian_kernel(x, y, sigma):\n",
      "    return np.exp(-linalg.norm(x-y)**2 / (2 * (sigma ** 2)))\n",
      "\n",
      "\n",
      "def svmfit(X, y, kernel, PK, C=None):\n",
      "    n_samples, n_features = X.shape\n",
      "    if C is not None: C = float(C)\n",
      "    # Gram matrix\n",
      "    K = np.zeros((n_samples, n_samples))\n",
      "    for i in range(n_samples):\n",
      "        for j in range(n_samples):\n",
      "            if kernel == 'linear_kernel':\n",
      "                K[i,j] = linear_kernel(X[i], X[j])\n",
      "            if kernel == 'gaussian_kernel':\n",
      "                K[i,j] = gaussian_kernel(X[i], X[j],PK)\n",
      "            if kernel == 'polynomial_kernel':\n",
      "                K[i,j] = polynomial_kernel(X[i], X[j],PK)\n",
      "\n",
      "    P = cvxopt.matrix(np.outer(y,y) * K)\n",
      "    q = cvxopt.matrix(np.ones(n_samples) * -1)\n",
      "    A = cvxopt.matrix(y, (1,n_samples))\n",
      "    b = cvxopt.matrix(0.0)\n",
      "\n",
      "    if C is None:\n",
      "        G = cvxopt.matrix(np.diag(np.ones(n_samples) * -1))\n",
      "        h = cvxopt.matrix(np.zeros(n_samples))\n",
      "    else:\n",
      "        tmp1 = np.diag(np.ones(n_samples) * -1)\n",
      "        tmp2 = np.identity(n_samples)\n",
      "        G = cvxopt.matrix(np.vstack((tmp1, tmp2)))\n",
      "        tmp1 = np.zeros(n_samples)\n",
      "        tmp2 = np.ones(n_samples) * C\n",
      "        h = cvxopt.matrix(np.hstack((tmp1, tmp2)))\n",
      "\n",
      "    # solve QP problem\n",
      "    cvxopt.solvers.options['show_progress'] = False\n",
      "    solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
      "\n",
      "    # Lagrange multipliers\n",
      "    a = np.ravel(solution['x'])\n",
      "    \n",
      "    # Support vectors have non zero lagrange multipliers\n",
      "    tol = 1.0e-5\n",
      "    idx = np.where(a>tol) ## Indice donde multiplicadores distintos de cero\n",
      "    idx=idx[0]\n",
      "    a = a[idx]            ## Multiplicadores distintos de cero\n",
      "    sv_x = X[idx]         ## Elementos X \n",
      "    sv_y = y[idx]         ## Elementos Y \n",
      "    \n",
      "    #print \"%d support vectors out of %d points\" % (len(a), n_samples)\n",
      "\n",
      "    # Intercept\n",
      "    b = 0\n",
      "    for n in range(len(a)):\n",
      "        b += sv_y[n]\n",
      "        b -= np.sum(a * sv_y * K[idx[n],idx])\n",
      "    b /= len(a)\n",
      "\n",
      "    # Weight vector\n",
      "    if kernel == linear_kernel:\n",
      "        w = np.zeros(n_features)\n",
      "        for n in range(len(self.a)):\n",
      "            w += a[n] * sv_y[n] * sv[n]\n",
      "    else:\n",
      "        w = None\n",
      "        \n",
      "    return a,sv_y,sv_x,w,b\n",
      "    \n",
      "\n",
      "def project(X, kernel, a, sv_y, sv, w, b, PK):\n",
      "    if w is not None:\n",
      "        return np.dot(X, w) + b\n",
      "    else:\n",
      "        a = np.array(a)[np.newaxis]\n",
      "        sv_y = np.array(sv_y)[np.newaxis]\n",
      "        \n",
      "        y_predict = np.zeros(len(X))\n",
      "        for i in range(len(X)):\n",
      "            for j in range(len(sv)):\n",
      "                s = 0\n",
      "                if kernel == 'linear_kernel':\n",
      "                    s += a.T[j]*sv_y.T[j]*linear_kernel(X[i],sv[j]);\n",
      "                if kernel == 'gaussian_kernel':\n",
      "                    s += a.T[j]*sv_y.T[j]*gaussian_kernel(X[i],sv[j], PK);\n",
      "                if kernel == 'polynomial_kernel':\n",
      "                    s += a.T[j]*sv_y.T[j]*polynomial_kernel(X[i],sv[j], PK);\n",
      "            y_predict[i] = s\n",
      "        return y_predict + b\n",
      "\n",
      "def predict(X, kernel, a, sv_y, sv, w, b, PK):\n",
      "    return np.sign(project(X, kernel, a, sv_y, sv, w, b, PK))\n",
      "\n",
      "def loadDataFrame(filename):\n",
      "    ##Load data from file return dataFrame\n",
      "    mat = spio.loadmat(filename,squeeze_me=True)\n",
      "    datos = mat['datos'].tolist()\n",
      "    X=datos[0];Y=datos[1];\n",
      "    A=np.insert(X, X.shape[1], Y, axis=1)\n",
      "    idx = range(0, A.shape[0])\n",
      "    idy = list(range(0, A.shape[1]-1))\n",
      "    idy.append('label')\n",
      "    df = pd.DataFrame.from_records(A, columns=(idy), index=idx)\n",
      "    df.columns=['a','b','c','d','e','f','g','h','i','j','label']\n",
      "    return df\n",
      "\n",
      "def SplitDataFrame(data, Ntrain):\n",
      "    ## Suffle rows in DataFrame\n",
      "    dfr = data.reindex(np.random.permutation(data.index))\n",
      "    X = dfr.values[:,0:10]\n",
      "    Y = dfr.values[:,10]\n",
      "    N=X.shape[0]\n",
      "    Ntest = N - Ntrain\n",
      "    ## Normalization\n",
      "    X = preprocessing.scale(X)\n",
      "        \n",
      "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=Ntest, random_state=42)\n",
      "    return (X_train, X_test, Y_train, Y_test)\n",
      "\n",
      "def getAccuracy(X_test,Y_test, pred):\n",
      "    #testSet=np.c_[ X_test, Y_test ]\n",
      "    correct = incorrect = 0\n",
      "    for x in range(len(Y_test)):\n",
      "        #if testSet[x][-1] == pred[x]:\n",
      "        if Y_test[x] == pred[x]:\n",
      "            correct += 1\n",
      "        else:\n",
      "            incorrect += 1\n",
      "        accuracy=(correct/float(len(Y_test))) * 100.0\n",
      "        error=(incorrect/float(len(Y_test))) * 100.0\n",
      "    return (accuracy, error)\n",
      "\n",
      "def main():\n",
      "    # Get Data\n",
      "    filename='/home/bgx/trabajofinaldeprctica/datosOS14.mat'\n",
      "    DF_data=loadDataFrame(filename)\n",
      "\n",
      "    ## Sample train SIZE\n",
      "    SIZE=80\n",
      "\n",
      "    ## Get Xtrain, Xtest, Ytrain, Ytest\n",
      "    Xtrain,Xtest,Ytrain,Ytest=SplitDataFrame(DF_data, SIZE)\n",
      "    \n",
      "    ## Cambio el nombre de la segunda clase\n",
      "    Ytrain[Ytrain==2] = -1\n",
      "    Ytest[Ytest==2]= -1\n",
      "\n",
      "    ## Optimo usando el CrossValidation en el TrainSet\n",
      "    X=Xtrain\n",
      "    Y=Ytrain\n",
      "    \n",
      "    ## KFold divides all the samples in math:K groups of samples, called folds .\n",
      "    #Simple K-Fold cross validation. 10 folds. Return Indexes for Xtrain and Ytrain\n",
      "    # Kfold = N => Leave One Out strategy\n",
      "    folds=2\n",
      "    kf_total = cross_validation.KFold(len(X), n_folds=folds, shuffle=True, random_state=8)\n",
      "    #Kernel='linear_kernel'\n",
      "    Kernel='gaussian_kernel'\n",
      "    #Kernel='polynomial_kernel'\n",
      "    ####Kernel polinomico evaluar el grado\n",
      "    \n",
      "    #PK = 1.5\n",
      "    Err_PK_C=[]\n",
      "    PK=[0.1,0.2,0.5,1.0,1.5,2]\n",
      "    PC=[0.1,0.2,0.5,1.0,1.5,2]\n",
      "    for i in range(0,len(PK),1):\n",
      "        Err_C = np.array([])\n",
      "        for j in range(0,len(PC),1):\n",
      "            Err_val = np.array([])\n",
      "            for train, test in kf_total:\n",
      "                A, SV_Y, SV, W, B = svmfit(X[train], Y[train],Kernel,PK[i], PC[j])  \n",
      "                ## Test Classifier Over KFold\n",
      "                if Kernel == 'linear_kernel':\n",
      "                    hat = predict(X[test], Kernel, A, SV_Y, SV, W, B, PK = False)\n",
      "                    correct = np.sum(hat == Y[test])\n",
      "                    #print \"%d out of %d predictions correct\" % (correct, len(hat))\n",
      "                    accuracy, error = getAccuracy(X[test],Y[test], hat)\n",
      "                    #print('Accuracy: ' + repr(accuracy) + '%')\n",
      "                if Kernel == 'gaussian_kernel':\n",
      "                    hat = predict(X[test], Kernel, A, SV_Y, SV, W, B, PK[i])\n",
      "                    correct = np.sum(hat == Y[test])\n",
      "                    #print \"%d out of %d predictions correct\" % (correct, len(hat))\n",
      "                    accuracy, error = getAccuracy(X[test],Y[test], hat)\n",
      "                    #print('Accuracy: ' + repr(accuracy) + '%')\n",
      "                if Kernel == 'polynomial_kernel':\n",
      "                    hat = predict(X[test], Kernel, A, SV_Y, SV, W, B, PK[i])\n",
      "                    correct = np.sum(hat == Y[test])\n",
      "                    #print \"%d out of %d predictions correct\" % (correct, len(hat))\n",
      "                    accuracy, error = getAccuracy(X[test],Y[test], hat)\n",
      "                    #print('Accuracy: ' + repr(accuracy) + '%')\n",
      "                Err_val = np.append(Err_val,error)\n",
      "            meanEval=np.mean(Err_val,axis=0)\n",
      "            Err_C = np.append(Err_C,meanEval)\n",
      "        Err_PK_C.append(Err_C)\n",
      "    PKC = np.vstack(Err_PK_C)\n",
      "    \n",
      "    ##Find Optimal PK and C\n",
      "    minid = np.where(PKC == PKC.min())\n",
      "    PKopt=PK[minid[0][0]]\n",
      "    PCopt=PC[minid[1][0]]\n",
      "    \n",
      "    A, SV_Y, SV, W, B = svmfit(Xtrain, Ytrain,Kernel,PKopt, PCopt)\n",
      "        \n",
      "    Yhat = predict(Xtest, Kernel, A, SV_Y, SV, W, B,PKopt)\n",
      "    correct = np.sum(Yhat == Ytest)\n",
      "    accuracy, error = getAccuracy(Xtest,Ytest, Yhat)\n",
      "    print('Accuracy: ' + repr(accuracy) + '%')\n",
      "\n",
      "\n",
      "main()\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy: 45.0%\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}